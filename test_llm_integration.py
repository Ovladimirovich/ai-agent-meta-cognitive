#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
"""

import asyncio
import os
import sys
from integrations.llm_client import create_llm_client


async def test_llm_integration():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å LLM"""

    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ API –∫–ª—é—á–∏
    available_keys = []
    key_names = {
        "OPENAI_API_KEY": "OpenAI",
        "ANTHROPIC_API_KEY": "Anthropic",
        "GOOGLE_API_KEY": "Google",
        "GROK_API_KEY": "Grok",
        "TOGETHER_API_KEY": "Together AI"
    }

    for env_var, name in key_names.items():
        if os.getenv(env_var):
            available_keys.append((env_var, name))

    if not available_keys:
        print("‚ùå API –∫–ª—é—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("\nüí° –ù–∞—Å—Ç—Ä–æ–π—Ç–µ API –∫–ª—é—á–∏ –≤ —Ñ–∞–π–ª–µ .env:")
        for env_var, name in key_names.items():
            print(f"   {env_var} - –¥–ª—è {name}")
        print("\nüìñ –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ .env.example")
        return

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(available_keys)} API –∫–ª—é—á–µ–π:")
    for env_var, name in available_keys:
        print(f"   ‚Ä¢ {name}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–ª—é—á
    env_var, provider_name = available_keys[0]
    print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {provider_name}...")

    try:
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç–∞
        client = await create_llm_client(
            provider=env_var.replace("_API_KEY", "").lower(),
            api_key=os.getenv(env_var),
            temperature=0.7,
            max_tokens=500
        )

        # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        test_prompt = "–ü—Ä–∏–≤–µ—Ç! –¢—ã AI –∞–≥–µ–Ω—Ç —Å –º–µ—Ç–∞-–ø–æ–∑–Ω–∞–Ω–∏–µ–º. –ö—Ä–∞—Ç–∫–æ —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–≤–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."

        print(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {test_prompt[:50]}...")

        response = await client.generate_response(
            prompt=test_prompt,
            system_message="–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."
        )

        print("‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω!")
        print(f"ü§ñ –ú–æ–¥–µ–ª—å: {response['model']} ({response['provider']})")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response['confidence']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {response['processing_time']:.2f} —Å–µ–∫")
        print(f"üí¨ –û—Ç–≤–µ—Ç: {response['content']}")

        if 'usage' in response and response['usage']:
            usage = response['usage']
            print(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: {usage.get('total_tokens', 'N/A')}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        await client.__aexit__(None, None, None)

        print("\nüéâ LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ {provider_name}: {e}")
        print("\nüîß –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
        print("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å API –∫–ª—é—á–∞")
        print("   ‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è")
        print("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–∏–º–∏—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API")
        print("   ‚Ä¢ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä")


async def test_fallback_responses():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ fallback –æ—Ç–≤–µ—Ç–æ–≤"""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ fallback –æ—Ç–≤–µ—Ç–æ–≤ (–±–µ–∑ API –∫–ª—é—á–µ–π)")
    print("=" * 50)

    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º agent_core –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è fallback
    from agent.core.agent_core import AgentCore
    from agent.core.models import AgentConfig

    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –±–µ–∑ LLM
    config = AgentConfig(
        max_execution_time=30.0,
        confidence_threshold=0.5,
        enable_reasoning_trace=True,
        enable_memory=False,
        max_memory_entries=100,
        tool_timeout=10.0
    )

    agent = AgentCore(config)

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–ø—Ä–∏–≤–µ—Ç",
        "—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å",
        "–∫–∞–∫ –¥–µ–ª–∞",
        "—Ä–∞—Å—Å–∫–∞–∂–∏ –æ python"
    ]

    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ fallback –æ—Ç–≤–µ—Ç–æ–≤:")

    for query in test_queries:
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            from agent.core.models import AgentRequest
            request = AgentRequest(
                id=f"test_{hash(query)}",
                query=query,
                user_id="test_user",
                session_id="test_session",
                timestamp=None
            )

            response = await agent._generate_fallback_response(request)
            print(f"   ‚ùì '{query}' ‚Üí '{response[:50]}...'")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å –∑–∞–ø—Ä–æ—Å–æ–º '{query}': {e}")

    print("\n‚úÖ Fallback –æ—Ç–≤–µ—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç!")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ AI Agent LLM Integration Test")
    print("=" * 50)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
    await test_llm_integration()

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º fallback
    await test_fallback_responses()

    print("\n" + "=" * 50)
    print("‚ú® –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("\nüí° –î–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–≥–µ–Ω—Ç–∞:")
    print("   python -m uvicorn api.main:app --reload")
    print("\nüåê Frontend: http://localhost:3000")
    print("üîó API: http://localhost:8000")


if __name__ == "__main__":
    asyncio.run(main())
