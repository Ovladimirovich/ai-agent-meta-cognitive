import logging
import gc
import psutil
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from collections import OrderedDict
import threading
import time
from weakref import WeakValueDictionary

from ..core.models import MemoryEntry

logger = logging.getLogger("MemoryManager")


class MemoryEntryMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ –ø–∞–º—è—Ç–∏"""

    def __init__(self, entry_id: str, size_bytes: int, importance: float = 0.5,
                 access_count: int = 0, last_access: datetime = None):
        self.entry_id = entry_id
        self.size_bytes = size_bytes
        self.importance = importance
        self.access_count = access_count
        self.last_access = last_access or datetime.now()
        self.created_at = datetime.now()

    def update_access(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ—Å—Ç—É–ø–∞"""
        self.access_count += 1
        self.last_access = datetime.now()

    def get_age_days(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∑–∞–ø–∏—Å–∏ –≤ –¥–Ω—è—Ö"""
        return (datetime.now() - self.created_at).total_seconds() / (24 * 3600)

    def get_inactivity_days(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–Ω–µ–π –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞"""
        return (datetime.now() - self.last_access).total_seconds() / (24 * 360)

    def calculate_relevance_score(self) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏"""
        # –§–æ—Ä–º—É–ª–∞: importance * (1 / (1 + age)) * (1 / (1 + inactivity)) * access_bonus
        age_penalty = 1 / (1 + self.get_age_days())
        inactivity_penalty = 1 / (1 + self.get_inactivity_days())
        access_bonus = min(2.0, 1 + (self.access_count * 0.1))

        return self.importance * age_penalty * inactivity_penalty * access_bonus


class MemoryPool:
    """–ü—É–ª –ø–∞–º—è—Ç–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""

    def __init__(self, max_size_mb: float = 100.0, cleanup_threshold: float = 0.8):
        self.max_size_bytes = int(max_size_mb * 1024 * 1024)
        self.cleanup_threshold = cleanup_threshold  # –ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—á–∏—Å—Ç–∫–∏
        self.current_size_bytes = 0
        self.entries: OrderedDict[str, Any] = OrderedDict()
        self.metadata: Dict[str, MemoryEntryMetadata] = {}
        self.lock = threading.RLock()
        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π
        self.expired_entries: Set[str] = set()
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ—á–∏—Å—Ç–∫–∏
        self.last_cleanup_time = datetime.now()

    def add_entry(self, entry_id: str, data: Any, importance: float = 0.5) -> bool:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –ø—É–ª

        Args:
            entry_id: ID –∑–∞–ø–∏—Å–∏
            data: –î–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            importance: –í–∞–∂–Ω–æ—Å—Ç—å (0.0-1.0)

        Returns:
            True –µ—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ, False –µ—Å–ª–∏ –Ω–µ—Ç –º–µ—Å—Ç–∞
        """
        with self.lock:
            # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
            size_bytes = self._estimate_size(data)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –º–µ—Å—Ç–∞
            if self.current_size_bytes + size_bytes > self.max_size_bytes:
                # –ü–æ–ø—ã—Ç–∫–∞ –æ—á–∏—Å—Ç–∫–∏
                if not self._cleanup_space(size_bytes):
                    return False

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏
            self.entries[entry_id] = data
            self.metadata[entry_id] = MemoryEntryMetadata(
                entry_id=entry_id,
                size_bytes=size_bytes,
                importance=importance
            )
            self.current_size_bytes += size_bytes

            return True

    def get_entry(self, entry_id: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏"""
        with self.lock:
            if entry_id in self.entries:
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ—Å—Ç—É–ø–∞
                if entry_id in self.metadata:
                    self.metadata[entry_id].update_access()
                    # –£–±–∏—Ä–∞–µ–º –∏–∑ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö, –µ—Å–ª–∏ –±—ã–ª —Ç–∞–º
                    self.expired_entries.discard(entry_id)
                return self.entries[entry_id]
            return None

    def remove_entry(self, entry_id: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏"""
        with self.lock:
            if entry_id in self.entries:
                size_bytes = self.metadata[entry_id].size_bytes
                del self.entries[entry_id]
                del self.metadata[entry_id]
                self.current_size_bytes -= size_bytes
                # –£–±–∏—Ä–∞–µ–º –∏–∑ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö, –µ—Å–ª–∏ –±—ã–ª —Ç–∞–º
                self.expired_entries.discard(entry_id)
                return True
            return False

    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—É–ª–∞"""
        with self.lock:
            return {
                "current_size_mb": self.current_size_bytes / (1024 * 1024),
                "max_size_mb": self.max_size_bytes / (1024 * 1024),
                "utilization_percent": (self.current_size_bytes / self.max_size_bytes) * 100,
                "entry_count": len(self.entries),
                "cleanup_threshold": self.cleanup_threshold * 10,
                "expired_entries_count": len(self.expired_entries)
            }

    def _estimate_size(self, data: Any) -> int:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–π—Ç–∞—Ö"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—á–µ—Ç–æ–≤
            if isinstance(data, (str, bytes)):
                return len(data.encode('utf-8') if isinstance(data, str) else data)
            elif isinstance(data, dict):
                return sum(self._estimate_size(v) for v in data.values())
            elif isinstance(data, (list, tuple)):
                return sum(self._estimate_size(item) for item in data)
            else:
                # –î–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
                return 1024 # 1KB –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        except:
            return 1024  # Fallback

    def _cleanup_space(self, required_bytes: int) -> bool:
        """
        –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Returns:
            True –µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—Ç–∞
        """
        with self.lock:
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∑–∞–ø–∏—Å–µ–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–æ—Ç –º–µ–Ω–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫ –±–æ–ª–µ–µ)
            entries_by_relevance = sorted(
                self.metadata.items(),
                key=lambda x: x[1].calculate_relevance_score()
            )

            freed_bytes = 0
            removed_count = 0

            for entry_id, metadata in entries_by_relevance:
                if freed_bytes >= required_bytes:
                    break

                # –ù–µ —É–¥–∞–ª—è–µ–º –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã–µ –∑–∞–ø–∏—Å–∏
                if metadata.importance >= 0.8:
                    continue

                # –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏
                self.remove_entry(entry_id)
                freed_bytes += metadata.size_bytes
                removed_count += 1

            logger.info(f"Memory pool cleanup: freed {freed_bytes} bytes, removed {removed_count} entries")

            return freed_bytes >= required_bytes

    def cleanup_expired_entries(self, max_age_days: float = 30.0, force_cleanup: bool = False):
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏"""
        with self.lock:
            current_time = datetime.now()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–æ—à–ª–æ –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ—á–∏—Å—Ç–∫–∏
            if not force_cleanup and (current_time - self.last_cleanup_time).total_seconds() < 300:  # 5 –º–∏–Ω—É—Ç
                return

            expired_ids = []
            for entry_id, metadata in self.metadata.items():
                if metadata.get_age_days() > max_age_days and metadata.importance < 0.7:
                    expired_ids.append(entry_id)
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö
                    self.expired_entries.add(entry_id)

            # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏
            for entry_id in expired_ids:
                self.remove_entry(entry_id)

            if expired_ids:
                logger.info(f"Cleaned up {len(expired_ids)} expired memory entries")
                self.last_cleanup_time = current_time

    def cleanup_inactive_entries(self, max_inactivity_days: float = 7.0, min_importance: float = 0.3):
        """–û—á–∏—Å—Ç–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏"""
        with self.lock:
            inactive_ids = []
            for entry_id, metadata in self.metadata.items():
                if metadata.get_inactivity_days() > max_inactivity_days and metadata.importance < min_importance:
                    inactive_ids.append(entry_id)

            for entry_id in inactive_ids:
                self.remove_entry(entry_id)

            if inactive_ids:
                logger.info(f"Cleaned up {len(inactive_ids)} inactive memory entries")

    def cleanup_memory_pressure(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –¥–∞–≤–ª–µ–Ω–∏–∏ –Ω–∞ –ø–∞–º—è—Ç—å"""
        with self.lock:
            utilization = self.current_size_bytes / self.max_size_bytes
            if utilization > 0.9:  # –ï—Å–ª–∏ –ø–∞–º—è—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 90%
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–∞–∏–º–µ–Ω—å—à–µ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
                entries_by_importance = sorted(
                    self.metadata.items(),
                    key=lambda x: x[1].importance
                )

                freed_bytes = 0
                removed_count = 0

                for entry_id, metadata in entries_by_importance:
                    if self.current_size_bytes / self.max_size_bytes <= 0.7:  # –¶–µ–ª—å - —Å–Ω–∏–∑–∏—Ç—å –¥–æ 70%
                        break

                    # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –Ω–∏–∑–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
                    if metadata.importance < 0.5:
                        size_to_free = metadata.size_bytes
                        self.remove_entry(entry_id)
                        freed_bytes += size_to_free
                        removed_count += 1

                if removed_count > 0:
                    logger.info(f"Memory pressure cleanup: freed {freed_bytes} bytes, removed {removed_count} entries")


class MemoryManager:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ —Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º —É—Ç–µ—á–µ–∫ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.

    –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
    - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º –ø–∞–º—è—Ç–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
    - –ó–∞—â–∏—Ç–∞ –æ—Ç —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏
    - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """

    def __init__(self, max_entries: int = 1000, max_working_memory_mb: float = 50.0, max_semantic_memory_mb: float = 100.0):
        # –ü—É–ª—ã –ø–∞–º—è—Ç–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
        self.episodic_pool = MemoryPool(max_size_mb=max_semantic_memory_mb)
        self.working_pool = MemoryPool(max_size_mb=max_working_memory_mb)
        self.semantic_pool = MemoryPool(max_size_mb=max_semantic_memory_mb)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.max_entries = max_entries

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.memory_stats = {
            "total_operations": 0,
            "cleanup_operations": 0,
            "memory_warnings": 0,
            "last_cleanup": datetime.now(),
            "start_time": datetime.now()
        }

        # Weak references –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
        self._weak_references = WeakValueDictionary()

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–µ—Ä –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏
        self._cleanup_timer = None

        logger.info(f"üöÄ MemoryManager initialized with memory limits: episodic={max_semantic_memory_mb}MB, working={max_working_memory_mb}MB, semantic={max_semantic_memory_mb}MB")

    async def store_episodic_memory(self, memory_data: Dict[str, Any]):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π

        Args:
            memory_data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        try:
            self.memory_stats["total_operations"] += 1

            # –°–æ–∑–¥–∞–Ω–∏–µ ID –¥–ª—è –∑–∞–ø–∏—Å–∏
            entry_id = f"episodic_{int(time.time() * 1000000)}_{hash(str(memory_data)) % 10000}"

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
            confidence = memory_data.get("confidence", 0.0)
            importance = min(1.0, max(0.1, confidence))  # –í–∞–∂–Ω–æ—Å—Ç—å –æ—Ç 0.1 –¥–æ 1.0 –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

            # –ü–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ø—É–ª
            if self.episodic_pool.add_entry(entry_id, memory_data, importance):
                logger.debug(f"Stored episodic memory: {entry_id}, importance={importance:.2f}")
            else:
                logger.warning(f"Failed to store episodic memory: insufficient space")

        except Exception as e:
            logger.error(f"‚ùå Failed to store episodic memory: {e}")

    def store_working_memory(self, key: str, value: Any, ttl_seconds: Optional[int] = None, importance: float = 0.7):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏ —Å TTL

        Args:
            key: –ö–ª—é—á
            value: –ó–Ω–∞—á–µ–Ω–∏–µ
            ttl_seconds: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ (None - –±–µ—Å—Å—Ä–æ—á–Ω–æ)
            importance: –í–∞–∂–Ω–æ—Å—Ç—å
        """
        try:
            self.memory_stats["total_operations"] += 1

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ TTL –≤ –¥–∞–Ω–Ω—ã–µ
            enriched_value = {
                "data": value,
                "expires_at": (datetime.now() + timedelta(seconds=ttl_seconds)).isoformat() if ttl_seconds else None,
                "stored_at": datetime.now().isoformat()
            }

            if self.working_pool.add_entry(key, enriched_value, importance):
                logger.debug(f"Stored working memory: {key}")
            else:
                logger.warning(f"Failed to store working memory: {key} - insufficient space")

        except Exception as e:
            logger.error(f"‚ùå Failed to store working memory: {e}")

    def retrieve_working_memory(self, key: str) -> Optional[Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π TTL

        Args:
            key: –ö–ª—é—á

        Returns:
            –ó–Ω–∞—á–µ–Ω–∏–µ –∏–ª–∏ None
        """
        try:
            enriched_value = self.working_pool.get_entry(key)
            if not enriched_value:
                return None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ TTL
            if enriched_value.get("expires_at"):
                expires_at = datetime.fromisoformat(enriched_value["expires_at"])
                if datetime.now() > expires_at:
                    # –£–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ç–µ–∫—à–µ–π –∑–∞–ø–∏—Å–∏
                    self.working_pool.remove_entry(key)
                    logger.debug(f"Working memory entry expired: {key}")
                    return None

            return enriched_value["data"]

        except Exception as e:
            logger.error(f"‚ùå Failed to retrieve working memory: {e}")
            return None

    def retrieve_episodic_memory(self, limit: int = 10, min_importance: float = 0.0) -> List[MemoryEntry]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π

        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
            min_importance: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –ø–∞–º—è—Ç–∏
        """
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevant_entries = []

        for entry_id, metadata in self.episodic_pool.metadata.items():
            if metadata.importance >= min_importance:
                data = self.episodic_pool.get_entry(entry_id)
                if data:
                    relevant_entries.append((data, metadata.calculate_relevance_score()))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-N
        relevant_entries.sort(key=lambda x: x[1], reverse=True)
        return [data for data, score in relevant_entries[:limit]]

    def find_similar_episodes(self, current_query: str, limit: int = 5) -> List[MemoryEntry]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤ –≤ –ø–∞–º—è—Ç–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.

        Args:
            current_query: –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        """
        if not self.episodic_pool.entries:
            return []

        similar_episodes = []
        current_words = set(current_query.lower().split())

        for entry_id, metadata in self.episodic_pool.metadata.items():
            entry = self.episodic_pool.get_entry(entry_id)
            if not entry or not entry.get("request") or not entry["request"].get("query"):
                continue

            # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—é —Å–ª–æ–≤
            entry_words = set(entry["request"]["query"].lower().split())
            intersection = current_words.intersection(entry_words)

            if intersection:
                similarity_score = len(intersection) / len(current_words.union(entry_words))

                if similarity_score > 0.1:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                    similar_episodes.append((entry, similarity_score, metadata.calculate_relevance_score()))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-N
        similar_episodes.sort(key=lambda x: x[2], reverse=True)  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        return [entry for entry, similarity, relevance in similar_episodes[:limit]]

    def get_memory_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏

        Returns:
            –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ—Ç –≤—Å–µ—Ö –ø—É–ª–æ–≤
        episodic_stats = self.episodic_pool.get_stats()
        working_stats = self.working_pool.get_stats()
        semantic_stats = self.semantic_pool.get_stats()

        # –°–∏—Å—Ç–µ–º–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        process = psutil.Process(os.getpid())
        system_memory = process.memory_info()

        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        total_utilization = (
            episodic_stats["utilization_percent"] +
            working_stats["utilization_percent"] +
            semantic_stats["utilization_percent"]
        ) / 3

        return {
            "pools": {
                "episodic": episodic_stats,
                "working": working_stats,
                "semantic": semantic_stats
            },
            "system": {
                "rss_mb": system_memory.rss / (1024 * 1024),
                "vms_mb": system_memory.vms / (1024 * 1024),
                "cpu_percent": process.cpu_percent(interval=0.1)
            },
            "operations": {
                "total": self.memory_stats["total_operations"],
                "cleanup_count": self.memory_stats["cleanup_operations"],
                "warnings": self.memory_stats["memory_warnings"]
            },
            "performance": {
                "average_utilization_percent": total_utilization,
                "uptime_hours": (datetime.now() - self.memory_stats["start_time"]).total_seconds() / 3600,
                "last_cleanup": self.memory_stats["last_cleanup"].isoformat()
            }
        }

    def _cleanup_expired_working_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–µ–∫—à–∏—Ö –∑–∞–ø–∏—Å–µ–π —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏"""
        # –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ø—É–ª–µ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏
        self.working_pool.cleanup_expired_entries(max_age_days=1.0)  # 1 –¥–µ–Ω—å

    def _cleanup_old_entries(self):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏"""
        # –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ø—É–ª–µ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏
        self.episodic_pool.cleanup_expired_entries(max_age_days=7.0)  # 7 –¥–Ω–µ–π

    def _cleanup_inactive_entries(self):
        """–û—á–∏—Å—Ç–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏"""
        # –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤–æ –≤—Å–µ—Ö –ø—É–ª–∞—Ö
        self.episodic_pool.cleanup_inactive_entries(max_inactivity_days=14.0, min_importance=0.2)
        self.working_pool.cleanup_inactive_entries(max_inactivity_days=1.0, min_importance=0.3)
        self.semantic_pool.cleanup_inactive_entries(max_inactivity_days=30.0, min_importance=0.4)

    def _cleanup_memory_pressure(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –¥–∞–≤–ª–µ–Ω–∏–∏ –Ω–∞ –ø–∞–º—è—Ç—å"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –ø–∞–º—è—Ç—å –≤–æ –≤—Å–µ—Ö –ø—É–ª–∞—Ö
        self.episodic_pool.cleanup_memory_pressure()
        self.working_pool.cleanup_memory_pressure()
        self.semantic_pool.cleanup_memory_pressure()

    def perform_memory_cleanup(self, force_cleanup: bool = False):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫"""
        try:
            # –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π
            self.episodic_pool.cleanup_expired_entries(max_age_days=7.0, force_cleanup=force_cleanup)
            self.working_pool.cleanup_expired_entries(max_age_days=1.0, force_cleanup=force_cleanup)
            self.semantic_pool.cleanup_expired_entries(max_age_days=30.0, force_cleanup=force_cleanup)

            # –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            self._cleanup_inactive_entries()

            # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –¥–∞–≤–ª–µ–Ω–∏–∏ –Ω–∞ –ø–∞–º—è—Ç—å
            self._cleanup_memory_pressure()

            # –í—ã–∑–æ–≤ —Å–±–æ—Ä—â–∏–∫–∞ –º—É—Å–æ—Ä–∞
            gc.collect()

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.memory_stats["cleanup_operations"] += 1
            self.memory_stats["last_cleanup"] = datetime.now()

            logger.info("Memory cleanup completed successfully")
        except Exception as e:
            logger.error(f"Error during memory cleanup: {e}")

    def clear_memory(self, memory_type: str = "all"):
        """
        –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)

        Args:
            memory_type: –¢–∏–ø –ø–∞–º—è—Ç–∏ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ (working, episodic, semantic, all)
        """
        if memory_type in ["working", "all"]:
            # –û—á–∏—Å—Ç–∫–∞ working –ø—É–ª–∞
            entry_ids = list(self.working_pool.entries.keys())
            for entry_id in entry_ids:
                self.working_pool.remove_entry(entry_id)
            logger.info("Working memory cleared")

        if memory_type in ["episodic", "all"]:
            # –û—á–∏—Å—Ç–∫–∞ episodic –ø—É–ª–∞
            entry_ids = list(self.episodic_pool.entries.keys())
            for entry_id in entry_ids:
                self.episodic_pool.remove_entry(entry_id)
            logger.info("Episodic memory cleared")

        if memory_type in ["semantic", "all"]:
            # –û—á–∏—Å—Ç–∫–∞ semantic –ø—É–ª–∞
            entry_ids = list(self.semantic_pool.entries.keys())
            for entry_id in entry_ids:
                self.semantic_pool.remove_entry(entry_id)
            logger.info("Semantic memory cleared")

    def export_memory(self, format: str = "dict") -> Any:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –ø–∞–º—è—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö.

        Args:
            format: –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ (dict, json)

        Returns:
            –î–∞–Ω–Ω—ã–µ –ø–∞–º—è—Ç–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        memory_data = {
            "episodic_memory": [self.episodic_pool.get_entry(entry_id) for entry_id in self.episodic_pool.entries.keys()],
            "working_memory": {key: self.working_pool.get_entry(key) for key in self.working_pool.entries.keys()},
            "semantic_memory": {key: self.semantic_pool.get_entry(key) for key in self.semantic_pool.entries.keys()},
            "stats": self.get_memory_stats(),
            "export_timestamp": datetime.now().isoformat()
        }

        if format == "json":
            # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é
            return memory_data
        else:
            return memory_data

    def import_memory(self, memory_data: Dict[str, Any]):
        """
        –ò–º–ø–æ—Ä—Ç –ø–∞–º—è—Ç–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è).

        Args:
            memory_data: –î–∞–Ω–Ω—ã–µ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
        """
        try:
            # –ò–º–ø–æ—Ä—Ç —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏
            if "episodic_memory" in memory_data:
                for i, entry_data in enumerate(memory_data["episodic_memory"]):
                    entry_id = f"episodic_imported_{i}"
                    importance = entry_data.get("confidence", 0.5)
                    self.episodic_pool.add_entry(entry_id, entry_data, importance)

            # –ò–º–ø–æ—Ä—Ç —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏
            if "working_memory" in memory_data:
                for key, value in memory_data["working_memory"].items():
                    importance = 0.7  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    if isinstance(value, dict) and "data" in value:
                        importance = value.get("importance", 0.7)
                    self.working_pool.add_entry(key, value, importance)

            # –ò–º–ø–æ—Ä—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏
            if "semantic_memory" in memory_data:
                for key, value in memory_data["semantic_memory"].items():
                    importance = 0.8  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏
                    self.semantic_pool.add_entry(key, value, importance)

            logger.info("Memory imported successfully")

        except Exception as e:
            logger.error(f"‚ùå Failed to import memory: {e}")

    def get_learning_insights(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.

        Returns:
            –ò–Ω—Å–∞–π—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞
        """
        insights = {
            "total_experiences": len(self.episodic_pool.entries),
            "average_confidence": 0.0,
            "common_intents": {},
            "strategy_effectiveness": {},
            "performance_trends": []
        }

        if not self.episodic_pool.entries:
            return insights

        # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        total_confidence = 0.0
        confidence_count = 0

        intent_counts = {}
        strategy_stats = {}

        for entry_id, metadata in self.episodic_pool.metadata.items():
            entry = self.episodic_pool.get_entry(entry_id)
            if not entry:
                continue

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = entry.get("confidence", 0.0)
            if confidence > 0:
                total_confidence += confidence
                confidence_count += 1

            # –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏–π
            if entry.get("analysis") and entry["analysis"].get("intent"):
                intent = entry["analysis"]["intent"]
                intent_counts[intent] = intent_counts.get(intent, 0) + 1

            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
            strategy = entry.get("strategy", "unknown")
            if strategy not in strategy_stats:
                strategy_stats[strategy] = {"count": 0, "total_confidence": 0.0}

            strategy_stats[strategy]["count"] += 1
            strategy_stats[strategy]["total_confidence"] += confidence

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if confidence_count > 0:
            insights["average_confidence"] = total_confidence / confidence_count

        insights["common_intents"] = intent_counts

        # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
        for strategy, stats in strategy_stats.items():
            stats["average_confidence"] = stats["total_confidence"] / stats["count"]

        insights["strategy_effectiveness"] = strategy_stats

        return insights

    def retrieve_semantic_memory(self, query: str, limit: int = 5) -> List[Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏
        """
        try:
            # –ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—É–ª–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É
            results = []
            query_lower = query.lower()
            
            for entry_id, metadata in self.semantic_pool.metadata.items():
                entry_data = self.semantic_pool.get_entry(entry_id)
                if entry_data:
                    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –∑–∞–ø–∏—Å–∏
                    entry_str = str(entry_data).lower()
                    if query_lower in entry_str or query_lower.replace(" ", "") in entry_str.replace(" ", ""):
                        results.append((entry_data, metadata.calculate_relevance_score()))
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results.sort(key=lambda x: x[1], reverse=True)
            return [item[0] for item in results[:limit]]
            
        except Exception as e:
            logger.error(f"‚ùå Failed to retrieve semantic memory: {e}")
            return []
