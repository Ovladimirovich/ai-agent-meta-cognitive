"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
"""

import logging
import asyncio
import gc
import psutil
import os
from typing import Dict, List, Any, Optional, Tuple, AsyncIterator
from datetime import datetime, timedelta
from collections import deque, OrderedDict
import threading
import time
from weakref import WeakValueDictionary
import pickle
import zlib
from concurrent.futures import ThreadPoolExecutor

from ..core.models import MemoryEntry

logger = logging.getLogger("AsyncMemoryManager")

class AsyncMemoryEntry:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –ø–∞–º—è—Ç–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Å–∂–∞—Ç–∏—è"""
    
    def __init__(self, entry_id: str, data: Any, importance: float = 0.5, tags: Optional[List[str]] = None):
        self.entry_id = entry_id
        self.data = data
        self.importance = importance
        self.tags = tags or []
        self.created_at = datetime.now()
        self.access_count = 0
        self.last_access = datetime.now()
        self.size_bytes = self._estimate_size(data)
        self.is_compressed = False

    def _estimate_size(self, data: Any) -> int:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–π—Ç–∞—Ö"""
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            serialized = pickle.dumps(data)
            return len(serialized)
        except:
            # –†–µ–∑–µ—Ä–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            if isinstance(data, (str, bytes)):
                return len(data.encode('utf-8') if isinstance(data, str) else data)
            elif isinstance(data, dict):
                return sum(self._estimate_size(v) for v in data.values())
            elif isinstance(data, (list, tuple)):
                return sum(self._estimate_size(item) for item in data)
            else:
                return 1024  # 1KB –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def compress(self) -> bool:
        """–°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"""
        if self.is_compressed:
            return True
            
        try:
            serialized_data = pickle.dumps(self.data)
            compressed_data = zlib.compress(serialized_data)
            # –¢–æ–ª—å–∫–æ —Å–∂–∏–º–∞–µ–º, –µ—Å–ª–∏ –ø–æ–ª—É—á–∞–µ–º —ç–∫–æ–Ω–æ–º–∏—é > 20%
            if len(compressed_data) < len(serialized_data) * 0.8:
                self.data = compressed_data
                self.is_compressed = True
                return True
            return False
        except Exception as e:
            logger.warning(f"Failed to compress entry {self.entry_id}: {e}")
            return False

    def decompress(self) -> bool:
        """–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if not self.is_compressed:
            return True
            
        try:
            decompressed_data = zlib.decompress(self.data)
            self.data = pickle.loads(decompressed_data)
            self.is_compressed = False
            return True
        except Exception as e:
            logger.error(f"Failed to decompress entry {self.entry_id}: {e}")
            return False

    def update_access(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ—Å—Ç—É–ø–∞"""
        self.access_count += 1
        self.last_access = datetime.now()

    def get_age_days(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∑–∞–ø–∏—Å–∏ –≤ –¥–Ω—è—Ö"""
        return (datetime.now() - self.created_at).total_seconds() / (24 * 3600)

    def get_inactivity_days(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–Ω–µ–π –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞"""
        return (datetime.now() - self.last_access).total_seconds() / (24 * 3600)

    def calculate_relevance_score(self) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏"""
        # –§–æ—Ä–º—É–ª–∞: importance * (1 / (1 + age)) * (1 / (1 + inactivity)) * access_bonus
        age_penalty = 1 / (1 + self.get_age_days())
        inactivity_penalty = 1 / (1 + self.get_inactivity_days())
        access_bonus = min(2.0, 1 + (self.access_count * 0.1))

        return self.importance * age_penalty * inactivity_penalty * access_bonus

class AsyncMemoryPool:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø—É–ª –ø–∞–º—è—Ç–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–∂–∞—Ç–∏—è –∏ –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, max_size_mb: float = 100.0, cleanup_threshold: float = 0.8, compression_threshold: float = 0.7):
        self.max_size_bytes = int(max_size_mb * 1024 * 1024)
        self.cleanup_threshold = cleanup_threshold  # –ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—á–∏—Å—Ç–∫–∏
        self.compression_threshold = compression_threshold  # –ü–æ—Ä–æ–≥ –¥–ª—è —Å–∂–∞—Ç–∏—è
        self.current_size_bytes = 0
        self.entries: OrderedDict[str, AsyncMemoryEntry] = OrderedDict()
        self.lock = asyncio.Lock()
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.compression_enabled = True

    async def add_entry(self, entry_id: str, data: Any, importance: float = 0.5, tags: Optional[List[str]] = None) -> bool:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –ø—É–ª

        Args:
            entry_id: ID –∑–∞–ø–∏—Å–∏
            data: –î–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            importance: –í–∞–∂–Ω–æ—Å—Ç—å (0.0-1.0)
            tags: –¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

        Returns:
            True –µ—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ, False –µ—Å–ª–∏ –Ω–µ—Ç –º–µ—Å—Ç–∞
        """
        async with self.lock:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏
            entry = AsyncMemoryEntry(entry_id, data, importance, tags)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –º–µ—Å—Ç–∞
            if self.current_size_bytes + entry.size_bytes > self.max_size_bytes:
                # –ü–æ–ø—ã—Ç–∫–∞ –æ—á–∏—Å—Ç–∫–∏
                if not await self._cleanup_space(entry.size_bytes):
                    return False

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏
            self.entries[entry_id] = entry
            self.current_size_bytes += entry.size_bytes

            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if self.compression_enabled and self._should_compress():
                asyncio.create_task(self._compress_entries())

            return True

    async def get_entry(self, entry_id: str) -> Optional[Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏"""
        async with self.lock:
            if entry_id in self.entries:
                entry = self.entries[entry_id]
                entry.update_access()
                
                # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                if entry.is_compressed:
                    entry.decompress()
                
                return entry.data
            return None

    async def remove_entry(self, entry_id: str) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏"""
        async with self.lock:
            if entry_id in self.entries:
                entry = self.entries[entry_id]
                size_bytes = entry.size_bytes
                del self.entries[entry_id]
                self.current_size_bytes -= size_bytes
                return True
            return False

    async def get_entries_by_tag(self, tag: str) -> List[Tuple[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –ø–æ —Ç–µ–≥—É"""
        async with self.lock:
            results = []
            for entry_id, entry in self.entries.items():
                if tag in entry.tags:
                    # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                    if entry.is_compressed:
                        entry.decompress()
                    results.append((entry_id, entry.data))
            return results

    async def get_stats(self) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—É–ª–∞"""
        async with self.lock:
            compressed_count = sum(1 for entry in self.entries.values() if entry.is_compressed)
            return {
                "current_size_mb": self.current_size_bytes / (1024 * 1024),
                "max_size_mb": self.max_size_bytes / (1024 * 1024),
                "utilization_percent": (self.current_size_bytes / self.max_size_bytes) * 100,
                "entry_count": len(self.entries),
                "compressed_entries": compressed_count,
                "cleanup_threshold": self.cleanup_threshold * 100,
                "compression_enabled": self.compression_enabled
            }

    async def _cleanup_space(self, required_bytes: int) -> bool:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Returns:
            True –µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—Ç–∞
        """
        async with self.lock:
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∑–∞–ø–∏—Å–µ–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–æ—Ç –º–µ–Ω–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫ –±–æ–ª–µ–µ)
            entries_by_relevance = sorted(
                self.entries.items(),
                key=lambda x: x[1].calculate_relevance_score()
            )

            freed_bytes = 0
            removed_count = 0

            for entry_id, entry in entries_by_relevance:
                if freed_bytes >= required_bytes:
                    break

                # –ù–µ —É–¥–∞–ª—è–µ–º –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã–µ –∑–∞–ø–∏—Å–∏
                if entry.importance >= 0.8:
                    continue

                # –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏
                del self.entries[entry_id]
                freed_bytes += entry.size_bytes
                removed_count += 1

            logger.info(f"Memory pool cleanup: freed {freed_bytes} bytes, removed {removed_count} entries")

            return freed_bytes >= required_bytes

    def _should_compress(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–∂–∞—Ç–∏—è"""
        utilization = self.current_size_bytes / self.max_size_bytes
        return utilization > self.compression_threshold

    async def _compress_entries(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∑–∞–ø–∏—Å–µ–π"""
        async with self.lock:
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –¥–∞–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            entries_to_compress = sorted(
                self.entries.items(),
                key=lambda x: (x[1].size_bytes, x[1].get_inactivity_days()),
                reverse=True
            )[:10]  # –°–∂–∏–º–∞–µ–º —Ç–æ–ª—å–∫–æ 10 —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö/—Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
            
            for entry_id, entry in entries_to_compress:
                if not entry.is_compressed and entry.size_bytes > 1024:  # > 1KB
                    entry.compress()
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è
                    if entry.is_compressed:
                        self.current_size_bytes -= (entry.size_bytes - len(entry.data))

    async def cleanup_expired_entries(self, max_age_days: float = 30.0):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π"""
        async with self.lock:
            expired_ids = []
            for entry_id, entry in self.entries.items():
                if entry.get_age_days() > max_age_days and entry.importance < 0.7:
                    expired_ids.append(entry_id)

            for entry_id in expired_ids:
                await self.remove_entry(entry_id)

            if expired_ids:
                logger.info(f"Cleaned up {len(expired_ids)} expired memory entries")

    async def clear(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π"""
        async with self.lock:
            self.entries.clear()
            self.current_size_bytes = 0

class AsyncMemoryManager:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö.
    
    –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
    - –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–∂–∞—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö
    - –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    - –°—Ç—Ä–∏–º–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
    """

    def __init__(self,
                 max_episodic_entries: int = 1000,
                 max_working_memory_mb: float = 50.0,
                 max_semantic_memory_mb: float = 100.0,
                 cleanup_interval_seconds: int = 300):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–∞–º—è—Ç–∏

        Args:
            max_episodic_entries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π
            max_working_memory_mb: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏ (MB)
            max_semantic_memory_mb: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ (MB)
            cleanup_interval_seconds: –ò–Ω—Ç–µ—Ä–≤–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ (—Å–µ–∫—É–Ω–¥—ã)
        """
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –ø—É–ª—ã –ø–∞–º—è—Ç–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
        self.episodic_pool = AsyncMemoryPool(max_size_mb=max_semantic_memory_mb)
        self.working_pool = AsyncMemoryPool(max_size_mb=max_working_memory_mb)
        self.semantic_pool = AsyncMemoryPool(max_size_mb=max_semantic_memory_mb)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.max_episodic_entries = max_episodic_entries

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.memory_stats = {
            "total_operations": 0,
            "cleanup_operations": 0,
            "memory_warnings": 0,
            "last_cleanup": datetime.now(),
            "start_time": datetime.now()
        }

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
        self.cleanup_interval = cleanup_interval_seconds
        self._cleanup_task = None

        # Weak references –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
        self._weak_references = WeakValueDictionary()

        logger.info("üöÄ AsyncMemoryManager initialized with async memory pools")

    async def start_cleanup_task(self):
        """–ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –æ—á–∏—Å—Ç–∫–∏"""
        if self._cleanup_task is None:
            self._cleanup_task = asyncio.create_task(self._cleanup_worker())
            logger.info("Started async memory cleanup task")

    async def stop_cleanup_task(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –æ—á–∏—Å—Ç–∫–∏"""
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
            self._cleanup_task = None
            logger.info("Stopped async memory cleanup task")

    # ===== –ê–°–ò–ù–•–†–û–ù–ù–ê–Ø –≠–ü–ò–ó–û–î–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ =====

    async def store_episodic_memory(self, memory_data: Dict[str, Any], importance: float = 0.5, tags: Optional[List[str]] = None):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π

        Args:
            memory_data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            importance: –í–∞–∂–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ (0.0-1.0)
            tags: –¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        try:
            self.memory_stats["total_operations"] += 1

            # –°–æ–∑–¥–∞–Ω–∏–µ ID –¥–ª—è –∑–∞–ø–∏—Å–∏
            entry_id = f"episodic_{int(time.time() * 1000000)}_{hash(str(memory_data)) % 10000}"

            # –ü–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ø—É–ª
            if await self.episodic_pool.add_entry(entry_id, memory_data, importance, tags):
                logger.debug(f"Stored episodic memory: {entry_id}")
            else:
                logger.warning(f"Failed to store episodic memory: insufficient space")

        except Exception as e:
            logger.error(f"‚ùå Failed to store episodic memory: {e}")

    async def retrieve_episodic_memory(self, limit: int = 10, min_importance: float = 0.0) -> List[Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π

        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
            min_importance: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –ø–∞–º—è—Ç–∏
        """
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevant_entries = []

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –ø—É–ª–∞ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        for entry_id in list(self.episodic_pool.entries.keys()):
            data = await self.episodic_pool.get_entry(entry_id)
            if data:
                entry = self.episodic_pool.entries[entry_id]
                if entry.importance >= min_importance:
                    relevant_entries.append((data, entry.calculate_relevance_score()))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-N
        relevant_entries.sort(key=lambda x: x[1], reverse=True)
        return [data for data, score in relevant_entries[:limit]]

    async def retrieve_episodic_memory_by_tag(self, tag: str, limit: int = 10) -> List[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –ø–æ —Ç–µ–≥—É"""
        entries = await self.episodic_pool.get_entries_by_tag(tag)
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        entries_with_relevance = []
        for entry_id, data in entries:
            entry = self.episodic_pool.entries[entry_id]
            entries_with_relevance.append((data, entry.calculate_relevance_score()))
        
        entries_with_relevance.sort(key=lambda x: x[1], reverse=True)
        return [data for data, score in entries_with_relevance[:limit]]

    # ===== –ê–°–ò–ù–•–†–û–ù–ù–ê–Ø –†–ê–ë–û–ß–ê–Ø –ü–ê–ú–Ø–¢–¨ =====

    async def store_working_memory(self, key: str, value: Any, ttl_seconds: Optional[int] = None, importance: float = 0.7, tags: Optional[List[str]] = None):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏ —Å TTL

        Args:
            key: –ö–ª—é—á
            value: –ó–Ω–∞—á–µ–Ω–∏–µ
            ttl_seconds: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ (None - –±–µ—Å—Å—Ä–æ—á–Ω–æ)
            importance: –í–∞–∂–Ω–æ—Å—Ç—å
            tags: –¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        try:
            self.memory_stats["total_operations"] += 1

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ TTL –≤ –¥–∞–Ω–Ω—ã–µ
            enriched_value = {
                "data": value,
                "expires_at": (datetime.now() + timedelta(seconds=ttl_seconds)).isoformat() if ttl_seconds else None,
                "stored_at": datetime.now().isoformat()
            }

            if await self.working_pool.add_entry(key, enriched_value, importance, tags):
                logger.debug(f"Stored working memory: {key}")
            else:
                logger.warning(f"Failed to store working memory: {key} - insufficient space")

        except Exception as e:
            logger.error(f"‚ùå Failed to store working memory: {e}")

    async def retrieve_working_memory(self, key: str) -> Optional[Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π TTL

        Args:
            key: –ö–ª—é—á

        Returns:
            –ó–Ω–∞—á–µ–Ω–∏–µ –∏–ª–∏ None
        """
        try:
            enriched_value = await self.working_pool.get_entry(key)
            if not enriched_value:
                return None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ TTL
            if enriched_value.get("expires_at"):
                expires_at = datetime.fromisoformat(enriched_value["expires_at"])
                if datetime.now() > expires_at:
                    # –£–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ç–µ–∫—à–µ–π –∑–∞–ø–∏—Å–∏
                    await self.working_pool.remove_entry(key)
                    logger.debug(f"Working memory entry expired: {key}")
                    return None

            return enriched_value["data"]

        except Exception as e:
            logger.error(f"‚ùå Failed to retrieve working memory: {e}")
            return None

    # ===== –ê–°–ò–ù–•–†–û–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ =====

    async def store_semantic_memory(self, key: str, value: Any, importance: float = 0.8, tags: Optional[List[str]] = None):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏

        Args:
            key: –ö–ª—é—á
            value: –ó–Ω–∞—á–µ–Ω–∏–µ
            importance: –í–∞–∂–Ω–æ—Å—Ç—å (–≤—ã—Å–æ–∫–∞—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏)
            tags: –¢–µ–≥–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        try:
            self.memory_stats["total_operations"] += 1

            if await self.semantic_pool.add_entry(key, value, importance, tags):
                logger.debug(f"Stored semantic memory: {key}")
            else:
                logger.warning(f"Failed to store semantic memory: {key} - insufficient space")

        except Exception as e:
            logger.error(f"‚ùå Failed to store semantic memory: {e}")

    async def retrieve_semantic_memory(self, key: str) -> Optional[Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏"""
        return await self.semantic_pool.get_entry(key)

    async def retrieve_semantic_memory_by_tag(self, tag: str, limit: int = 10) -> List[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –ø–æ —Ç–µ–≥—É"""
        entries = await self.semantic_pool.get_entries_by_tag(tag)
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        entries_with_relevance = []
        for entry_id, data in entries:
            entry = self.semantic_pool.entries[entry_id]
            entries_with_relevance.append((data, entry.calculate_relevance_score()))
        
        entries_with_relevance.sort(key=lambda x: x[1], reverse=True)
        return [data for data, score in entries_with_relevance[:limit]]

    # ===== –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ú–û–ù–ò–¢–û–†–ò–ù–ì –ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø =====

    async def get_memory_stats(self) -> Dict[str, Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏

        Returns:
            –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ—Ç –≤—Å–µ—Ö –ø—É–ª–æ–≤
        episodic_stats = await self.episodic_pool.get_stats()
        working_stats = await self.working_pool.get_stats()
        semantic_stats = await self.semantic_pool.get_stats()

        # –°–∏—Å—Ç–µ–º–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        process = psutil.Process(os.getpid())
        system_memory = process.memory_info()

        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        total_utilization = (
            episodic_stats["utilization_percent"] +
            working_stats["utilization_percent"] +
            semantic_stats["utilization_percent"]
        ) / 3

        return {
            "pools": {
                "episodic": episodic_stats,
                "working": working_stats,
                "semantic": semantic_stats
            },
            "system": {
                "rss_mb": system_memory.rss / (1024 * 1024),
                "vms_mb": system_memory.vms / (1024 * 1024),
                "cpu_percent": process.cpu_percent(interval=0.1)
            },
            "operations": {
                "total": self.memory_stats["total_operations"],
                "cleanup_count": self.memory_stats["cleanup_operations"],
                "warnings": self.memory_stats["memory_warnings"]
            },
            "performance": {
                "average_utilization_percent": total_utilization,
                "uptime_hours": (datetime.now() - self.memory_stats["start_time"]).total_seconds() / 3600,
                "last_cleanup": self.memory_stats["last_cleanup"].isoformat()
            }
        }

    async def optimize_memory(self) -> Dict[str, Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        logger.info("üîÑ Starting async memory optimization")

        results = {
            "freed_bytes": 0,
            "removed_entries": 0,
            "pools_optimized": 0,
            "gc_collections": 0
        }

        # –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π
        await self.episodic_pool.cleanup_expired_entries(max_age_days=7.0)  # –ù–µ–¥–µ–ª—è
        await self.working_pool.cleanup_expired_entries(max_age_days=1.0)   # –î–µ–Ω—å
        await self.semantic_pool.cleanup_expired_entries(max_age_days=30.0) # –ú–µ—Å—è—Ü

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –º—É—Å–æ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        def run_gc():
            return gc.collect()
        collected = await asyncio.get_event_loop().run_in_executor(None, run_gc)
        results["gc_collections"] = collected

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.memory_stats["cleanup_operations"] += 1
        self.memory_stats["last_cleanup"] = datetime.now()

        logger.info(f"‚úÖ Async memory optimization completed: {results}")
        return results

    async def check_memory_health(self) -> Dict[str, Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –ø–∞–º—è—Ç–∏

        Returns:
            –°—Ç–∞—Ç—É—Å –∑–¥–æ—Ä–æ–≤—å—è
        """
        stats = await self.get_memory_stats()

        health_status = "healthy"
        issues = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –ø—É–ª–æ–≤
        for pool_name, pool_stats in stats["pools"].items():
            if pool_stats["utilization_percent"] > 90:
                health_status = "critical"
                issues.append(f"{pool_name} pool over 90% full")
            elif pool_stats["utilization_percent"] > 75:
                if health_status == "healthy":
                    health_status = "warning"
                issues.append(f"{pool_name} pool over 75% full")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–π –ø–∞–º—è—Ç–∏
        if stats["system"]["rss_mb"] > 500:  # 500MB
            health_status = "critical"
            issues.append("High system memory usage")

        return {
            "status": health_status,
            "issues": issues,
            "recommendations": await self._get_memory_recommendations(health_status, issues)
        }

    async def _get_memory_recommendations(self, status: str, issues: List[str]) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏"""
        recommendations = []

        if status == "critical":
            recommendations.extend([
                "Immediate memory cleanup required",
                "Consider increasing memory limits",
                "Review memory-intensive operations"
            ])

        if status == "warning":
            recommendations.extend([
                "Schedule memory optimization",
                "Monitor memory usage trends",
                "Consider reducing cache sizes"
            ])

        for issue in issues:
            if "episodic" in issue:
                recommendations.append("Clean up old episodic memory entries")
            elif "working" in issue:
                recommendations.append("Clear expired working memory entries")
            elif "semantic" in issue:
                recommendations.append("Archive unused semantic memory")

        return recommendations

    # ===== –ê–°–ò–ù–•–†–û–ù–ù–´–ô –†–ê–ë–û–ß–ò–ô –¶–ò–ö–õ –û–ß–ò–°–¢–ö–ò =====

    async def _cleanup_worker(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–∞–±–æ—á–∏–π —Ü–∏–∫–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏"""
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                health = await self.check_memory_health()
                if health["status"] in ["warning", "critical"]:
                    logger.info("Running scheduled async memory optimization")
                    await self.optimize_memory()
                    self.memory_stats["cleanup_operations"] += 1

            except asyncio.CancelledError:
                logger.info("Async memory cleanup worker cancelled")
                break
            except Exception as e:
                logger.error(f"Error in async memory cleanup worker: {e}")
                await asyncio.sleep(60)  # –ü–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ

    # ===== –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨ –ò –£–¢–ò–õ–ò–¢–´ =====

    async def clear_memory(self, memory_type: str = "all"):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        if memory_type in ["episodic", "all"]:
            await self.episodic_pool.clear()

        if memory_type in ["working", "all"]:
            await self.working_pool.clear()

        if memory_type in ["semantic", "all"]:
            await self.semantic_pool.clear()

        logger.info(f"Memory cleared: {memory_type}")

    async def stream_memory_by_importance(self, min_importance: float = 0.5) -> AsyncIterator[Tuple[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å—Ç—Ä–∏–º–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        # –°—Ç—Ä–∏–º–∏–º —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫—É—é –ø–∞–º—è—Ç—å
        for entry_id in list(self.episodic_pool.entries.keys()):
            entry = self.episodic_pool.entries[entry_id]
            if entry.importance >= min_importance:
                data = await self.episodic_pool.get_entry(entry_id)
                if data:
                    yield ("episodic", entry_id, data)
        
        # –°—Ç—Ä–∏–º–∏–º —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å
        for entry_id in list(self.working_pool.entries.keys()):
            entry = self.working_pool.entries[entry_id]
            if entry.importance >= min_importance:
                data = await self.working_pool.get_entry(entry_id)
                if data:
                    yield ("working", entry_id, data)
                    
        # –°—Ç—Ä–∏–º–∏–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ø–∞–º—è—Ç—å
        for entry_id in list(self.semantic_pool.entries.keys()):
            entry = self.semantic_pool.entries[entry_id]
            if entry.importance >= min_importance:
                data = await self.semantic_pool.get_entry(entry_id)
                if data:
                    yield ("semantic", entry_id, data)

    def __str__(self) -> str:
        """–°—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"""
        # –î–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±–µ—Ä—Ç–∫—É
        import asyncio
        try:
            stats = asyncio.run(self.get_memory_stats())
            return (f"AsyncMemoryManager("
                    f"episodic: {stats['pools']['episodic']['utilization_percent']:.1f}%, "
                    f"working: {stats['pools']['working']['utilization_percent']:.1f}%, "
                    f"semantic: {stats['pools']['semantic']['utilization_percent']:.1f}%, "
                    f"system: {stats['system']['rss_mb']:.1f}MB)")
        except:
            return "AsyncMemoryManager(not initialized)"
