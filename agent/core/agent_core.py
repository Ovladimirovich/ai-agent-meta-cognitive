import logging
import time
from typing import Dict, Any, Optional

from datetime import datetime
from .models import (
    AgentConfig, AgentRequest, AgentResponse, AgentState,
    TaskComplexity, QueryAnalysis, ReasoningStep
)
from agent.tools.tool_orchestrator import ToolOrchestrator
from ..tools.query_analyzer import QueryAnalyzer
from ..memory.memory_manager import MemoryManager
from ..self_awareness.state_manager import StateManager
from ..self_awareness.confidence_calculator import ConfidenceCalculator
from ..self_awareness.reasoning_tracer import ReasoningTracer
from integrations.llm_client import create_llm_client, LLMProvider
from integrations.circuit_breaker import circuit_breaker_decorator, CircuitBreakerConfig

logger = logging.getLogger("AgentCore")


class AgentCore:
    """
    Ð¦ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚ AI Ð°Ð³ÐµÐ½Ñ‚Ð° Ñ Ð¼ÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼.

    ÐšÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÑÐµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð°Ð³ÐµÐ½Ñ‚Ð°: Ð°Ð½Ð°Ð»Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð², Ð²Ñ‹Ð±Ð¾Ñ€ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²,
    Ð¸ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡, ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð¸ Ð¼ÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸.
    """

    def __init__(self, config: AgentConfig):
        self.config = config

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²
        self.state_manager = StateManager()
        self.tool_orchestrator = ToolOrchestrator(config.agent_tool_timeout)
        self.memory_manager = MemoryManager(config.agent_max_memory_entries) if config.enable_memory else None
        self.query_analyzer = QueryAnalyzer()
        self.confidence_calculator = ConfidenceCalculator()
        self.reasoning_tracer = ReasoningTracer() if config.agent_enable_reasoning_trace else None

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ LLM ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° (Ð»ÐµÐ½Ð¸Ð²Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ°)
        self.llm_client = None
        self.llm_initialized = False

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        self.preload_manager = None
        if self.memory_manager:
            try:
                from .preload_manager import PreloadManager
                self.preload_manager = PreloadManager(self.memory_manager, config)
            except ImportError:
                logger.warning("PreloadManager not available, skipping initialization")
                self.preload_manager = None

        # ÐœÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹
        self.reasoning_trace: list[ReasoningStep] = []
        self.confidence_score: float = 0.0
        self.task_complexity: TaskComplexity = TaskComplexity.SIMPLE

        # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
        self.requests_processed = 0
        self.total_execution_time = 0.0
        self.error_count = 0
        self.last_execution_times = []
        self.tool_usage_stats = {}

        logger.info("ðŸš€ AgentCore initialized")

    async def _init_llm_client_async(self):
        """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ LLM ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°"""
        import os

        # ðŸ”¥ ÐŸÐ Ð˜ÐžÐ Ð˜Ð¢Ð•Ð¢: Ollama Ð²ÑÐµÐ³Ð´Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ÑÑ Ð¿ÐµÑ€Ð²Ñ‹Ð¼ (Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ð¾, Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾)
        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ URL Ollama Ð¸Ð· Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
            ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
            self.llm_client = await create_llm_client(
                provider="ollama",
                api_key=None,  # Ollama Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ API ÐºÐ»ÑŽÑ‡Ð°
                model="gemma3:1b",
                temperature=0.7,
                max_tokens=1000,
                base_url=ollama_url  # ÐŸÐµÑ€ÐµÐ´Ð°ÐµÐ¼ URL Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
            )
            logger.info("âœ… LLM client initialized with Ollama (Gemma3) - FREE local AI!")
            return

        except Exception as e:
            logger.warning(f"Ollama initialization failed: {e}. Trying cloud providers...")

        # Ð•ÑÐ»Ð¸ Ollama Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚, Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¾Ð±Ð»Ð°Ñ‡Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ñ‹
        api_configs = [
            ("MISTRAL_API_KEY", "mistral", "mistral-small"),  # Ð‘ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ñ‹Ð¹ tier
            ("GOOGLE_API_KEY", "google", "gemini-1.5-flash"),  # Ð‘ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ñ‹Ð¹ tier
            ("TOGETHER_API_KEY", "together", "mistralai/Mistral-7B-Instruct-v0.1"),  # ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ñ‹
            ("OPENAI_API_KEY", "openai", "gpt-3.5-turbo"),
            ("ANTHROPIC_API_KEY", "anthropic", "claude-3-haiku-20240307"),
            ("GROK_API_KEY", "grok", "grok-1"),
        ]

        for env_var, provider, model in api_configs:
            api_key = os.getenv(env_var)
            if api_key:
                try:
                    self.llm_client = await create_llm_client(
                        provider=provider,
                        api_key=api_key,
                        model=model,
                        temperature=0.7,
                        max_tokens=1000
                    )
                    logger.info(f"âœ… LLM client initialized with {provider} ({model})")
                    break

                except Exception as e:
                    logger.warning(f"Failed to initialize {provider} client: {e}")
                    continue

        if not self.llm_client:
            logger.warning("âš ï¸ No LLM API keys found. Using fallback responses.")
            logger.info("ðŸ’¡ To enable AI responses, set one of these FREE options:")
            logger.info("   ðŸ”¥ OLLAMA_URL - Ollama (FREE, local, best choice!)")
            logger.info("   MISTRAL_API_KEY - Mistral AI (FREE tier available)")
            logger.info("   GOOGLE_API_KEY - Google Gemini (FREE tier)")
            logger.info("   TOGETHER_API_KEY - Together AI (some FREE models)")
            logger.info("   Or paid options: OPENAI_API_KEY, ANTHROPIC_API_KEY, GROK_API_KEY")

    def _init_llm_client(self):
        """Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ° Ð´Ð»Ñ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð¹ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
        import asyncio
        try:
            asyncio.run(self._init_llm_client_async())
        except RuntimeError:
            # Ð•ÑÐ»Ð¸ ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ event loop, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ run_coroutine_threadsafe
            import threading
            if not hasattr(self, '_loop') or not self._loop.is_running():
                # Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ñ†Ð¸ÐºÐ»Ð°, ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ñ‚Ð¾Ðº
                def run_in_thread():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(self._init_llm_client_async())
                    finally:
                        loop.close()

                thread = threading.Thread(target=run_in_thread)
                thread.start()
                thread.join()

    @circuit_breaker_decorator("agent_core_process", CircuitBreakerConfig(
        failure_threshold=5,
        recovery_timeout=120.0,
        timeout=60.0,
        name="agent_core_process"
    ))
    async def process_request(self, request: AgentRequest) -> AgentResponse:
        """
        ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ñ Ð¼ÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼.

        ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚:
        1. ÐÐ½Ð°Ð»Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
        2. ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¸ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸
        3. Ð˜ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ñ Ñ‚Ñ€Ð°ÑÑÐ¸Ñ€Ð¾Ð²ÐºÐ¾Ð¹
        4. ÐžÑ†ÐµÐ½ÐºÐ° ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
        5. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ
        """
        start_time = time.time()

        try:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÑÑˆÐ° Ð¿ÐµÑ€ÐµÐ´ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹
            cache_key = self._get_cache_key(request.query, request.context)
            cached_result = await self._get_cached_response(cache_key)
            if cached_result:
                logger.info("âœ… Returning cached response")
                execution_time = time.time() - start_time
                return AgentResponse(
                    result=cached_result,
                    confidence=0.95,  # Ð’Ñ‹ÑÐ¾ÐºÐ¸Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²
                    reasoning_trace=[],
                    execution_time=execution_time
                )

            # ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
            preloaded_data = await self._get_preloaded_data(request.query)
            if preloaded_data:
                logger.info("âœ… Using preloaded data for faster response")
                execution_time = time.time() - start_time
                return AgentResponse(
                    result=preloaded_data,
                    confidence=0.9,
                    reasoning_trace=[],
                    execution_time=execution_time
                )

            # ÐŸÐµÑ€ÐµÑ…Ð¾Ð´ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
            self.state_manager.transition_to(AgentState.ANALYZING, "Starting request analysis")

            # Ð—Ð°Ð¿ÑƒÑÐº Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ
            if self.preload_manager and not self.preload_manager._running:
                await self.preload_manager.start_preloading()

            # ÐÐ½Ð°Ð»Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
            analysis = await self.query_analyzer.analyze(request)
            self._add_reasoning_step("analysis", "Query analyzed", {
                "intent": analysis.intent,
                "complexity": analysis.complexity.value,
                "required_tools": analysis.required_tools
            })

            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¸ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸
            self.task_complexity = analysis.complexity
            strategy = self._select_strategy(analysis)

            self._add_reasoning_step("strategy_selection", f"Selected strategy: {strategy}", {
                "complexity": self.task_complexity.value,
                "strategy": strategy
            })

            # ÐŸÐµÑ€ÐµÑ…Ð¾Ð´ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¸ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
            self.state_manager.transition_to(AgentState.EXECUTING, f"Executing with strategy: {strategy}")

            # Ð˜ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ñ Ñ‚Ñ€Ð°ÑÑÐ¸Ñ€Ð¾Ð²ÐºÐ¾Ð¹
            result = await self._execute_with_trace(request, strategy, analysis)

            # ÐžÑ†ÐµÐ½ÐºÐ° ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
            self.confidence_score = self.confidence_calculator.calculate(result, analysis)

            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ
            if self.memory_manager:
                await self.memory_manager.store_episodic_memory({
                    'request': request,
                    'analysis': analysis,
                    'strategy': strategy,
                    'result': result,
                    'confidence': self.confidence_score,
                    'execution_time': time.time() - start_time,
                    'timestamp': datetime.now()
                })

            execution_time = time.time() - start_time
            # Ð“Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¸Ð·Ð¼ÐµÑ€ÐµÐ½Ð¸Ð¹
            execution_time = max(execution_time, 0.001)

            # ÐšÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
            await self._cache_response(cache_key, result)

            # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
            self.requests_processed += 1
            self.total_execution_time += execution_time
            self.last_execution_times.append(execution_time)
            if len(self.last_execution_times) > 100:  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ
                self.last_execution_times.pop(0)

            # ÐŸÐµÑ€ÐµÑ…Ð¾Ð´ Ð² Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð½Ð¾Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ
            self.state_manager.transition_to(AgentState.COMPLETED, "Request processed successfully")

            self._add_reasoning_step("completion", "Request completed", {
                "confidence": self.confidence_score,
                "execution_time": execution_time
            })

            return AgentResponse(
                result=result,
                confidence=self.confidence_score,
                reasoning_trace=[step.dict() for step in self.reasoning_trace],
                execution_time=execution_time
            )

        except Exception as e:
            logger.error(f"âŒ Request processing failed: {e}")

            # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÑ‡ÐµÑ‚Ñ‡Ð¸ÐºÐ° Ð¾ÑˆÐ¸Ð±Ð¾Ðº
            self.error_count += 1

            # Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (Ð½Ðµ ÐºÐ¸Ð´Ð°ÐµÐ¼ Ð¸ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ…)
            try:
                self.state_manager.transition_to(AgentState.ERROR, str(e))
            except Exception as transition_error:
                logger.warning(f"Could not transition to ERROR state: {transition_error}")
                # Ð¡Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð² IDLE Ð´Ð»Ñ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶ÐµÐ½Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹
                try:
                    self.state_manager.transition_to(AgentState.IDLE, "Reset after error")
                except Exception as reset_error:
                    logger.error(f"Could not reset state: {reset_error}")

            # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ Ð½Ð¸Ð·ÐºÐ¾Ð¹ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒÑŽ
            execution_time = time.time() - start_time
            return AgentResponse(
                result=f"Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {str(e)}",
                confidence=0.1,
                reasoning_trace=[step.dict() for step in self.reasoning_trace],
                execution_time=execution_time,
                metadata={"error": str(e)}
            )

    def _select_strategy(self, analysis: QueryAnalysis) -> str:
        """Ð’Ñ‹Ð±Ð¾Ñ€ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ð¸ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°"""
        # Ð”Ð»Ñ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð²ÑÐµÐ³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÑÐ¼Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ñ‡ÐµÑ€ÐµÐ· LLM
        if analysis.complexity == TaskComplexity.SIMPLE:
            return "direct_response"
        elif analysis.intent in ["greeting", "casual_conversation"]:
            # ÐŸÑ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ Ð¸ casual Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€ Ð²ÑÐµÐ³Ð´Ð° Ñ‡ÐµÑ€ÐµÐ· LLM
            return "direct_response"
        elif analysis.complexity == TaskComplexity.MEDIUM:
            return "tool_assisted"
        else:  # COMPLEX
            return "multi_tool_pipeline"

    def _map_tool_names(self, tool_names: list[str]) -> list[str]:
        """ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð¼ÐµÐ½ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð¸Ð· query_analyzer Ð² Ð¸Ð¼ÐµÐ½Ð° tool_orchestrator"""
        name_mapping = {
            "rag_search": "rag",
            "data_analyzer": "analytics",
            "code_executor": "hybrid_models",
            "general_assistant": "hybrid_models",
            "reasoning_engine": "hybrid_models"
        }

        return [name_mapping.get(name, name) for name in tool_names]

    async def _execute_with_trace(self, request: AgentRequest, strategy: str, analysis: QueryAnalysis) -> Any:
        """Ð˜ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ñ Ñ‚Ñ€Ð°ÑÑÐ¸Ñ€Ð¾Ð²ÐºÐ¾Ð¹"""
        if strategy == "direct_response":
            # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð±ÐµÐ· Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            result = await self._generate_direct_response(request)
            self._add_reasoning_step("execution", "Direct response generated", {"strategy": strategy})

        elif strategy == "tool_assisted":
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°
            tool_names = self._map_tool_names(analysis.required_tools[:1])
            tool_results = await self.tool_orchestrator.execute_tools(tool_names, {
                "request": request,
                "analysis": analysis
            })
            result = self._process_tool_results(tool_results)
            self._add_reasoning_step("execution", f"Tool executed: {analysis.required_tools[0]}", {
                "strategy": strategy,
                "tools_used": analysis.required_tools[:1]
            })

        elif strategy == "multi_tool_pipeline":
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            tool_names = self._map_tool_names(analysis.required_tools)
            tool_results = await self.tool_orchestrator.execute_tools(tool_names, {
                "request": request,
                "analysis": analysis
            })
            result = self._process_tool_results(tool_results)
            self._add_reasoning_step("execution", f"Multiple tools executed: {analysis.required_tools}", {
                "strategy": strategy,
                "tools_used": analysis.required_tools
            })

        else:
            raise ValueError(f"Unknown strategy: {strategy}")

        return result

    async def _generate_direct_response(self, request: AgentRequest) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ LLM Ð´Ð»Ñ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²"""
        # Ð›ÐµÐ½Ð¸Ð²Ð°Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ LLM ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸
        if not self.llm_initialized:
            try:
                self._init_llm_client()
                self.llm_initialized = True
            except Exception as e:
                logger.warning(f"LLM initialization failed: {e}")

        if self.llm_client:
            try:
                # Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð° Ñ Ð¼ÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼
                system_message = (
                    "Ð¢Ñ‹ - AI Ð°Ð³ÐµÐ½Ñ‚ Ñ Ð¼ÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼. Ð¢Ñ‹ Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ²Ð¾Ð¸ Ð¼Ñ‹ÑÐ»Ð¸, "
                    "Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°Ñ‚ÑŒ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ð°Ñ… Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡. "
                    "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ. Ð‘ÑƒÐ´ÑŒ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¼, Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¼ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¼. "
                    "Ð•ÑÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð², Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ Ð¸Ñ… Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ."
                )

                # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· LLM
                response = await self.llm_client.generate_response(
                    prompt=request.query,
                    system_message=system_message
                )

                logger.info(f"LLM response generated - confidence: {response['confidence']}, "
                           f"model: {response['model']}, provider: {response['provider']}")

                return response["content"]

            except Exception as e:
                logger.error(f"LLM generation failed: {e}")
                # Fallback Ðº ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚Ð°Ð¼ Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ LLM
                return await self._generate_fallback_response(request)
        else:
            # Fallback ÐµÑÐ»Ð¸ LLM Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½
            return await self._generate_fallback_response(request)

    def _get_cache_key(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÐºÐ»ÑŽÑ‡Ð° Ð´Ð»Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²"""
        import hashlib
        cache_input = f"{query}_{str(context)}"
        return hashlib.md5(cache_input.encode()).hexdigest()

    async def _get_preloaded_data(self, query: str) -> Optional[str]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        if self.memory_manager:
            # Ð˜Ñ‰ÐµÐ¼ Ð² ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
            preloaded_memories = await self.memory_manager.retrieve_semantic_memory(query, limit=3)
            for memory in preloaded_memories:
                if hasattr(memory, 'metadata') and memory.metadata.get('preload_priority', 0) > 0:
                    return memory.content

        # Ð¢Ð°ÐºÐ¶Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        if self.preload_manager:
            preloaded_data = await self.preload_manager.get_preloaded_data(query)
            if preloaded_data:
                return preloaded_data

        return None

    async def _get_cached_response(self, cache_key: str) -> Optional[str]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¸Ð· ÐºÑÑˆÐ°"""
        # Fallback Ðº Ð¿Ð°Ð¼ÑÑ‚Ð¸
        if self.memory_manager:
            recent_memories = self.memory_manager.retrieve_episodic_memory(limit=5)
            for memory in recent_memories:
                if hasattr(memory, 'request') and hasattr(memory.request, 'query'):
                    if memory.request.query == cache_key:
                        return memory.result
        return None

    async def _cache_response(self, cache_key: str, response: str):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð² ÐºÑÑˆ"""
        # Fallback Ðº Ð¿Ð°Ð¼ÑÑ‚Ð¸
        if self.memory_manager and hasattr(self.memory_manager, 'store_working_memory'):
            try:
                await self.memory_manager.store_working_memory(cache_key, response, ttl_seconds=300)
            except Exception as e:
                logger.warning(f"Failed to cache response in memory: {e}")

    async def _generate_fallback_response(self, request: AgentRequest) -> str:
        """Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð±ÐµÐ· LLM"""
        query_lower = request.query.lower()

        # ÐŸÑ€Ð¾ÑÑ‚Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ fallback
        if any(word in query_lower for word in ["Ð¿Ñ€Ð¸Ð²ÐµÑ‚", "Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹", "Ð´Ð¾Ð±Ñ€Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ", "Ð´Ð¾Ð±Ñ€Ñ‹Ð¹ Ð²ÐµÑ‡ÐµÑ€", "Ð´Ð¾Ð±Ñ€Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾", "Ñ…Ð°Ð¹", "hello", "hi"]):
            return "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ AI Ð°Ð³ÐµÐ½Ñ‚ Ñ Ð¼ÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼. Ð¯ Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÑŒ Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð°Ð¼Ð¸. Ð§ÐµÐ¼ Ð¼Ð¾Ð³Ñƒ Ð±Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð»ÐµÐ·ÐµÐ½?"

        elif any(word in query_lower for word in ["ÐºÐ°Ðº Ð´ÐµÐ»Ð°", "ÐºÐ°Ðº Ð¿Ð¾Ð¶Ð¸Ð²Ð°ÐµÑˆÑŒ", "ÐºÐ°Ðº Ñ‚Ñ‹", "how are you"]):
            return "Ð£ Ð¼ÐµÐ½Ñ Ð²ÑÐµ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾! Ð¯ Ð¿Ð¾ÑÑ‚Ð¾ÑÐ½Ð½Ð¾ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑÑŒ Ð¸ ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²ÑƒÑŽÑÑŒ. Ð ÐºÐ°Ðº Ñƒ Ð²Ð°Ñ Ð´ÐµÐ»Ð°?"

        elif any(word in query_lower for word in ["Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ ÑƒÐ¼ÐµÐµÑˆÑŒ", "Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ Ð¼Ð¾Ð¶ÐµÑˆÑŒ", "Ñ‚Ð²Ð¾Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸", "what can you do"]):
            return ("Ð¯ AI Ð°Ð³ÐµÐ½Ñ‚ Ñ Ð¼ÐµÑ‚Ð°-Ð¿Ð¾Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼. ÐœÐ¾Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð²ÐºÐ»ÑŽÑ‡Ð°ÑŽÑ‚:\n"
                   "â€¢ ÐÐ½Ð°Ð»Ð¸Ð· Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²\n"
                   "â€¢ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡\n"
                   "â€¢ Ð¡Ð°Ð¼Ð¾Ð¾Ñ†ÐµÐ½ÐºÐ° ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ð°Ñ…\n"
                   "â€¢ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð¿Ñ‹Ñ‚Ð°\n"
                   "â€¢ Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼\n\n"
                   "Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ, Ð¸ Ñ Ð¿Ð¾ÑÑ‚Ð°Ñ€Ð°ÑŽÑÑŒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ!")

        elif any(word in query_lower for word in ["ÑÐ¿Ð°ÑÐ¸Ð±Ð¾", "Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€ÑŽ", "thanks", "thank you"]):
            return "ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°! Ð’ÑÐµÐ³Ð´Ð° Ñ€Ð°Ð´ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ. Ð•ÑÐ»Ð¸ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ½ÑƒÑ‚ ÐµÑ‰Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð¾Ð±Ñ€Ð°Ñ‰Ð°Ð¹Ñ‚ÐµÑÑŒ."

        elif any(word in query_lower for word in ["Ð¿Ð¾ÐºÐ°", "Ð´Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ", "bye", "goodbye"]):
            return "Ð”Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ! Ð‘Ñ‹Ð»Ð¾ Ð¿Ñ€Ð¸ÑÑ‚Ð½Ð¾ Ð¿Ð¾Ð¾Ð±Ñ‰Ð°Ñ‚ÑŒÑÑ. Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ð¹Ñ‚ÐµÑÑŒ, ÐºÐ¾Ð³Ð´Ð° Ð¿Ð¾Ð½Ð°Ð´Ð¾Ð±Ð¸Ñ‚ÑÑ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ."

        else:
            # Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð»Ñ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
            responses = [
                f"Ð¯ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð» Ð²Ð°Ñˆ Ð·Ð°Ð¿Ñ€Ð¾Ñ: '{request.query}'. Ð”Ð°Ð¹Ñ‚Ðµ Ð¼Ð½Ðµ Ð¿Ð¾Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ...",
                f"Ð˜Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ: '{request.query}'. Ð”Ð»Ñ Ð±Ð¾Ð»ÐµÐµ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¼Ð¾Ð¶ÐµÑ‚ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².",
                f"Ð’Ð°Ñˆ Ð·Ð°Ð¿Ñ€Ð¾Ñ: '{request.query}' Ð¿Ñ€Ð¸Ð½ÑÑ‚. Ð¯ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽ ÐµÐ³Ð¾ Ð¸ Ð¿Ð¾ÑÑ‚Ð°Ñ€Ð°ÑŽÑÑŒ Ð´Ð°Ñ‚ÑŒ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚.",
                f"Ð¡Ð¿Ð°ÑÐ¸Ð±Ð¾ Ð·Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ: '{request.query}'. Ð¯ AI Ð°Ð³ÐµÐ½Ñ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÑ‡Ð¸Ñ‚ÑÑ Ð¸ Ñ€Ð°Ð·Ð²Ð¸Ð²Ð°ÐµÑ‚ÑÑ. Ð”Ð°Ð¹Ñ‚Ðµ Ð¼Ð½Ðµ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð½Ð° Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ."
            ]

            # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð»Ñ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ
            import random
            return random.choice(responses)

    def _process_tool_results(self, tool_results: Dict[str, Any]) -> Any:
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²"""
        if not tool_results:
            return "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹"

        # Ð”Ð»Ñ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ñ‚Ñ‹ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð³Ð¾ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°
        for tool_name, result in tool_results.items():
            if hasattr(result, 'success') and result.success:
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚ result Ñƒ Ð¾Ð±ÑŠÐµÐºÑ‚Ð°
                if hasattr(result, 'result'):
                    return result.result
                else:
                    return "Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾"

        return "Ð’ÑÐµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð»Ð¸ÑÑŒ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸"

    def _add_reasoning_step(self, step_type: str, description: str, data: Optional[Dict[str, Any]] = None):
        """Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑˆÐ°Ð³Ð° Ð² Ñ‚Ñ€Ð°ÑÑÐ¸Ñ€Ð¾Ð²ÐºÑƒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹"""
        if self.reasoning_tracer:
            step = ReasoningStep(
                step_type=step_type,
                description=description,
                timestamp=datetime.now(),
                data=data or {}
            )
            self.reasoning_trace.append(step)
            self.reasoning_tracer.add_step(step_type, description, data)

    async def get_status(self) -> Dict[str, Any]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð°Ð³ÐµÐ½Ñ‚Ð°"""
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ð° Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
        orchestrator_status = await self.tool_orchestrator.get_status()

        return {
            "state": self.state_manager.current_state.value,
            "confidence": self.confidence_score,
            "task_complexity": self.task_complexity.value,
            "active_tools": orchestrator_status.get('total_tools', 0),
            "memory_entries": self.memory_manager.get_memory_stats() if self.memory_manager else 0,
            "reasoning_steps": len(self.reasoning_trace),
            "orchestrator_status": orchestrator_status
        }

    def get_metrics(self, timeframe: str = "1h") -> Dict[str, Any]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð°Ð³ÐµÐ½Ñ‚Ð°"""
        # Ð Ð°ÑÑ‡ÐµÑ‚ ÑÑ€ÐµÐ´Ð½Ð¸Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹
        avg_execution_time = 0.0
        if self.requests_processed > 0:
            avg_execution_time = self.total_execution_time / self.requests_processed

        error_rate = 0.0
        if self.requests_processed > 0:
            error_rate = self.error_count / self.requests_processed

        return {
            "requests_processed": self.requests_processed,
            "average_confidence": self.confidence_score,
            "average_execution_time": avg_execution_time,
            "error_rate": error_rate,
            "tool_usage_stats": self.tool_usage_stats.copy()
        }
