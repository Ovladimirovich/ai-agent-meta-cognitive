name: Performance Testing

on:
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: "0 2 * * *"
  workflow_dispatch: # Allow manual trigger
  push:
    branches: [main, develop]
    paths:
      - "agent/**"
      - "api/**"
      - "database/**"
      - "integrations/**"

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: ai_agent_test
          POSTGRES_USER: ai_agent
          POSTGRES_PASSWORD: test_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Run performance baseline tests
        run: |
          export POSTGRES_HOST=localhost
          export POSTGRES_PORT=5432
          export POSTGRES_DB=ai_agent_test
          export POSTGRES_USER=ai_agent
          export POSTGRES_PASSWORD=test_password
          export REDIS_HOST=localhost
          export REDIS_PORT=6379
          export ENVIRONMENT=test
          python -m pytest tests/performance_tests.py -v --tb=short

      - name: Run API performance tests
        run: |
          # Run the performance test script directly
          python tests/performance_tests.py

      - name: Run load tests
        run: |
          # Example: Using locust for load testing
          pip install locust
          echo "Running load tests would be implemented here"
          # locust -f tests/load_tests.py --headless -u 100 -r 10 -t 1m --host http://localhost:8000

      - name: Compare performance metrics
        run: |
          # This step would compare current performance with baseline
          echo "Comparing performance metrics..."
          # In a real implementation, you would compare current results with stored baselines
          # and fail the job if performance has degraded beyond acceptable thresholds

      - name: Store performance metrics
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics
          path: |
            tests/performance_baseline.json
            performance_results_*.txt
          retention-days: 30

  performance-monitoring:
    runs-on: ubuntu-latest
    needs: performance-test
    if: always()

    steps:
      - name: Download performance metrics
        uses: actions/download-artifact@v4
        with:
          name: performance-metrics
          path: ./performance-data

      - name: Analyze performance trends
        run: |
          echo "Analyzing performance trends..."
          # In a real implementation, you would analyze the performance data
          # and create reports or alerts if needed
          ls -la ./performance-data
